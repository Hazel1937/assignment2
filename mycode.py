# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VounZJsRX7nrybQ7hP22hE0o9ZSLBoMZ
"""

!pip install pyspark

from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("COMP5349 A2 Data Loading Example") \
    .getOrCreate()

test_data = "test.json"
test_init_df = spark.read.json(test_data)

test_init_df.printSchema()

from pyspark.sql.functions import explode
test_paragraph_df = test_init_df.select(explode("data.paragraphs").alias("paragraphs"))
test_paragraph_df.count()
test_context = test_paragraph_df.select('paragraphs.context',explode("paragraphs.qas").alias("qas"))
test_context.show(10)
test_context.printSchema()

def get_context_seq(context):
  end = 0
  seq_result = []
  while end < len(context):
    seq_result.append([context[end:end+4096], end, end+4096])
    end += 2048
  seq_result.append([context[end:], end, len(context)])
  return seq_result

def get_qas(qas):
  ans_result = []
  for qa in qas:
    question = qa['question']
    is_impossible = qa['is_impossible']
    for ans in qa['answer']:
      ans_start = ans['answer_start']
      ans_end = ans['answer_start'] + len(ans['text'])
      temp = [question, is_impossible, ans_start, ans_end]
      ans_result.append(temp)

def get_sample(context, qas):
  content_seq = get_context_seq(context)
  qas_seq = get_qas(qas)
  sample_result = []
  for content in content_seq:
    for qa in qa_seq:
      if qa[1]:
        sample_result.append({'source':content[0],'questiom':qa[0], 'start':0, 'end':0})
      else:
        if content[1] < qa[2] and content[2] > qa[2]:
          sample_result.append({'source':content,'questiom':qa[0], 'start':qa[2], 'end':qa[3]})
        else:
          sample_result.append({'source':content[0],'questiom':qa[0], 'start':0, 'end':0})
  return sample_result